{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.749 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5 3 ... 5 7 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         56602800  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 256)         384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 334, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 334, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 334, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 112, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 112, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 112, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7168)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7168)              28672     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1835264   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 59,041,656\n",
      "Trainable params: 59,027,320\n",
      "Non-trainable params: 14,336\n",
      "_________________________________________________________________\n",
      "(10800, 1000)\n",
      "(10800,)\n",
      "Epoch 1/10\n",
      "10800/10800 [==============================] - 9s 874us/step - loss: 2.8614 - acc: 0.2371\n",
      "Epoch 2/10\n",
      "10800/10800 [==============================] - 5s 501us/step - loss: 0.7903 - acc: 0.7486\n",
      "Epoch 3/10\n",
      "10800/10800 [==============================] - 5s 501us/step - loss: 0.1579 - acc: 0.9522\n",
      "Epoch 4/10\n",
      "10800/10800 [==============================] - 5s 502us/step - loss: 0.0293 - acc: 0.9927\n",
      "Epoch 5/10\n",
      "10800/10800 [==============================] - 5s 503us/step - loss: 0.0140 - acc: 0.9964\n",
      "Epoch 6/10\n",
      "10800/10800 [==============================] - 5s 503us/step - loss: 0.0131 - acc: 0.9969\n",
      "Epoch 7/10\n",
      "10800/10800 [==============================] - 5s 501us/step - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 8/10\n",
      "10800/10800 [==============================] - 5s 501us/step - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "10800/10800 [==============================] - 5s 500us/step - loss: 0.0010 - acc: 0.9999\n",
      "Epoch 10/10\n",
      "10800/10800 [==============================] - 5s 501us/step - loss: 0.0019 - acc: 0.9998\n",
      "[7 5 7 ... 5 5 6]\n",
      "accuracy:  0.9208333333333333\n",
      "F1-Score:  0.9209810515207596\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential, layers, utils, models, regularizers\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('chaobigyuliaoku.csv')\n",
    "cj = lambda x: list(jieba.cut(x))\n",
    "data['words'] = data['words'].apply(cj)\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(data['words'])\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "def yl_preprocessing():\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(data['label'].values)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data['words'].values, y, test_size=0.1, random_state=24)\n",
    "    x_train_wordids = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test_wordids = tokenizer.texts_to_sequences(x_test)\n",
    "    x_train_sequences = pad_sequences(x_train_wordids, maxlen=1000)\n",
    "    x_test_sequences = pad_sequences(x_test_wordids, maxlen=1000)\n",
    "    return x_train_sequences, x_test_sequences, y_train, y_test\n",
    "\n",
    "\n",
    "def CNN_model(x_train_sequences, x_test_sequences, y_train, y_test, vocab):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(len(vocab)+1, 300, input_length=1000))\n",
    "    model.add(layers.Conv1D(256, 5, padding='same'))\n",
    "    model.add(layers.MaxPooling1D(3, 3, padding='same'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(128, 5, padding='same'))\n",
    "    model.add(layers.MaxPooling1D(3, 3, padding='same'))\n",
    "    model.add(layers.Conv1D(64, 3, padding='same'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(8, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    one_hot_label = utils.to_categorical(y_train, num_classes=8)\n",
    "    # print(one_hot_label)\n",
    "    print(x_train_sequences.shape)\n",
    "    print(y_train.shape)\n",
    "    model.fit(x_train_sequences, one_hot_label, batch_size=500, epochs=10)\n",
    "    model.save('textcnn.h5')\n",
    "    y_predict = model.predict_classes(x_test_sequences)\n",
    "    print(y_predict)\n",
    "    print('accuracy: ', metrics.accuracy_score(y_test, y_predict))\n",
    "    print('F1-Score: ', metrics.f1_score(y_test, y_predict, average='weighted'))\n",
    "\n",
    "x_train_sequences, x_test_sequences, y_train, y_test = yl_preprocessing()\n",
    "print(y_train)\n",
    "CNN_model(x_train_sequences, x_test_sequences, y_train, y_test, vocab)\n",
    "\n",
    "model = models.load_model('textcnn.h5')\n",
    "test_x = []\n",
    "for word in jieba.lcut('6月27日晚，一则“成都某小区8岁男童被暴打至血流不止”的消息在不少群里流传。网友截屏显示，位于成都北门的某小区，一名少年将一名男童拖入地下室暴打两小时，致男童身受重伤，现场血迹斑斑。'):\n",
    "    test_x.append(vocab[word])\n",
    "word_pad = pad_sequences([test_x], maxlen=1000)\n",
    "print(model.predict_classes(word_pad))\n",
    "\n",
    "# ['体育' '娱乐' '房产' '教育' '时政' '社会' '科技' '财经']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.8",
   "language": "python",
   "name": "tensorflow-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
